\input{../preamble.tex}

\lecturenumber{1}
\title{File\\Systems}
\version{1.0.0}

\begin{document}

\begin{frame}[plain, noframenumbering]
    \titlepage
\end{frame}

\begin{slide}

    \slidetitle{What can happen on a crash?}

    Assume we are creating and writing to a new file /foo/bar

    Write in general (not just the first write)
    \medskip

    \includegraphics[width=130mm]{system-crash-1.png}

\end{slide}

\begin{slide}

    \slidetitle{What can happen on a crash?}
    
    Assume we are creating and writing to a new file /foo/bar
    \bigskip

    A crash after the completion of:
    \begin{itemize}
        \item Step   1-5: No corruption.
        \item Step     6: i-node bitmap points to garbage data.
        \item Step   7-9: Inconsistent metadata.
        \item Step 10-12: No corruption.  
        \item Step    13: data bitmap points to garbage data.
        \item Step    14: Corrupted data.
        \item Step    15: The new data cannot be found. 
    \end{itemize}

\end{slide}


\begin{slide}
    
    \slidetitle{What can happen on a crash?}

    \textbf{Lost Files:} New files being created might not be fully registered in the directory structure, leading to them being "lost" even if their data exists on disk.
    \bigskip

    \textbf{Inaccessible Files:} Existing files might become inaccessible due to corrupted metadata or lost pointers to their data blocks.
    \bigskip
    
    \textbf{Incorrect File Sizes:} The file system might report incorrect file sizes due to incomplete writes or metadata updates.
    \bigskip

    \textbf{Inconsistent Metadata:} Applications trying to access affected files might crash, freeze, or produce errors due to the inconsistencies.

\end{slide}

\begin{slide}

    \slidetitle{Recovery from failure}

    \textbf{Consistency checking} compares data in directory structure with data blocks on disk, and tries to fix inconsistencies.
    \begin{itemize}
        \item Tools: fsck for Unix systems, chkdsk for Windows.
        \item Scanning the entire disk, slow, very slow ...
    \end{itemize}
    \bigskip

    \textbf{Journaling:} A system that keeps track of changes that will be made in the file system to ensure data integrity.
    \bigskip

    \textbf{Versioning and Snapshots:} Maintain historical versions or states of files and systems to roll back when needed.
    \bigskip

    \textbf{Backup and Restore:} Periodic backups to external devices or cloud storage, ensuring data can be restored from a known good state.

\end{slide}

\begin{slide}

    \slidetitle{Journaling (a.k.a Write-Ahead Logging)} 

    Used by many modern file systems: 
    \begin{itemize}
        \item Linux (ext3, ext4), Windows (NTFS), IBM (JFS)
    \end{itemize}
    \bigskip

    First write down what you are going to do, and then carry out the action
    \bigskip

    \includegraphics[width=80mm]{journaling-1.png}
    \medskip

    \includegraphics[width=80mm]{journaling-2.png}

\end{slide}

\begin{slide}

    \slidetitle{Journaling (a.k.a Write-Ahead Logging)} 

    Physical journal
    \begin{itemize}
        \item all changed data is recorded in the journal first
    \end{itemize}
    \bigskip

    Logical journal
    \begin{itemize}
        \item only changed metadata (like inode tables, directory structures) is recorded
    \end{itemize}

\end{slide}

\begin{slide}

    \slidetitle{Journaling (a.k.a Write-Ahead Logging)} 

    What if absolute data integrity is crucial?
    \begin{itemize}
        \item for example, in banking, aviation systems, stock exchange?
    \end{itemize}
    \bigskip
    
    How do we ensure that data integrity is always maintained?

\end{slide}

\begin{slide}

    \slidetitle{ZFS}
    
    What happens if system crashes before each step can be completed?
    \bigskip

    \includegraphics[width=65mm]{zfs.jpg}

\end{slide}

\begin{slide}

	\slidetitle{Why distributed systems?}
	
	What if we need more performance than a single CPU?
	\bigskip
	
	Vertical Scaling: use more powerful systems / multiprocessors
	\begin{itemize}
		\item But these have scaling limits and cost \$
	\end{itemize}
	\bigskip
	
	Horizontal Scaling: distributed load across more systems
	\begin{itemize}
		\item Distributed systems allow us to achieve massive performance
	\end{itemize}

\end{slide}

\begin{slide}
	
	\slidetitle{Why distributed systems?}
	
	Cache data close to where it is needed.
	\bigskip
	
	Caching vs. replication
	\begin{itemize}
		\item Replication: multiple copies of data for increased fault tolerance
		\item Caching: temporary copies of frequently accessed data closer to where it’s needed.
	\end{itemize}
	
\end{slide}

\begin{slide}

    \slidetitle{Why distributed system? An example}

    2004: Facebook started on a single server
    \begin{itemize}
        \item Web server front end to assemble each user’s page. 
        \item Database to store posts, friend lists, etc.
    \end{itemize}
    \medskip

    2008: 100M users
    \medskip

    2010: 500M users
    \medskip

    2012: 1B users
    \medskip

    2019: 2.5B users
    \medskip

\end{slide}

\begin{slide}

    \slidetitle{An example scenario}

    For a web-based service to be available, both the server and the web server need to be operational.
    \bigskip

    In this scenario, both the server and the web server are hosted on a single machine.
    \bigskip

    This machine has a 10\% chance of failure.
    \bigskip

    What is the overall availability of the service, assuming no other components fail?

\end{slide}

\begin{slide}

    \slidetitle{An example scenario}

    Now the server and the web server are each hosted on a different machine.
    \bigskip

    Each machine has a 10\% chance of failure.
    \bigskip

    What is the overall availability of the service, assuming no other components fail?

\end{slide}

\begin{slide}

    \slidetitle{An example scenario}

    As the service grows, the user base expands, and more content is required.
    \bigskip

    To support this, 10 machines are dedicated to the servers, and another 10 are allocated for the web servers.
    \bigskip

    Each machine has a 10\% chance of failure.
    \bigskip

    What is the overall availability of the service, assuming no other components fail?

\end{slide}

\begin{slide}
    
    \slidetitle{An example scenario}

    The service provider aims to reduce their failure rate to below 1\% by creating replicas of their servers and web servers.
    \bigskip

    How many replicas are needed?

\end{slide}

\begin{slide}

    \slidetitle{Why distributed system?}

    Nature of the application
    \begin{itemize}
        \item Multiplayer games
        \item Collaborative Projects \& Cloud App
    \end{itemize}
    \bigskip

    Availability despite unreliable components 
    \begin{itemize}
        \item A service shouldn’t fail when one computer does. • 
    \end{itemize}
    \bigskip

    Conquer geographic separation 
    \begin{itemize}
        \item A web request in NZ is faster served by a server in NZ than by a server in US.
    \end{itemize}
    \bigskip

    Scale up capacity 
    \begin{itemize}
        \item More CPU cycles, more memory, more storage
    \end{itemize}

\end{slide}

\begin{slide}

    \slidetitle{What is a distributed system?}

    \begin{exampleblock}{}
          {\large ``A collection of loosely coupled nodes interconnected by a communication network.''}
            \vskip5mm
              \hspace*\fill{\small--- textbook}
    \end{exampleblock}

\end{slide}

\begin{slide}

    \slidetitle{What is a distributed system?}

    \includegraphics[width=65mm]{dfs-definition-1.png}
    \bigskip

    \textbf{Independent components or elements}
    \begin{itemize}
        \item software processes or hardware used to run a process, store data, etc.
    \end{itemize}

\end{slide}

\begin{slide}

    \slidetitle{What is a distributed system?}

    \includegraphics[width=65mm]{dfs-definition-2.png}
    \bigskip

    \textbf{Independent components or elements} that are \textbf{connected by a network.}

\end{slide}

\begin{slide}

    \slidetitle{What is a distributed system?}

    \includegraphics[width=80mm]{dfs-definition-4.png}
    \bigskip

    \textbf{Independent components or elements} that are \textbf{connected by a network} and \textbf{communicate by passing messages} to achieve a common goal, appearing as \textbf{a single coherent system}.
    \bigskip

    When one (or more) component fails, the system does not fail.

\end{slide}

\begin{slide}

	\slidetitle{Core challenges in distributed systems design}
	
	Cuncurrency
	\bigskip
	
	Latency
	\bigskip
	
	Partial Failure
	
\end{slide}

\begin{slide}

	\slidetitle{Concurrency}

	Lots of requests may occur at the same time.
	\bigskip
	
	Need to deal with concurrent requests.
	\begin{itemize}
		\item Need to ensure consistency of all data.
		\item Understand critical sections \& mutual exclusion.
		\item Beware: mutual exclusion (locking) can affect performance.
	\end{itemize}
	\bigskip
	
\end{slide}

\begin{slide}

	\slidetitle{Concurrency}

	We often replicate data (or cache it) – need to update all replicas
	\begin{itemize}
		\item Replication adds complexity
		\item All operations must appear to occur in the same order on all replicas
		\item Need to worry about out-of-order messages, undelivered messages, dead replicas
	\end{itemize}

\end{slide}

\begin{slide}

	\slidetitle{Latency}
	
	Network messages may take a long time to arrive.
	\bigskip
	
	Synchronous network model
	\begin{itemize}
		\item There is some upper bound, \textit{T}, between when a node sends a message and another node receives it.
		\item Knowing \textit{T} enables a node to distinguish between a node that has failed and a node that is taking a long time to respond.
	\end{itemize}
	
\end{slide}

\begin{slide}

	\slidetitle{Latency}
	
	Partially synchronous network model
	\begin{itemize}
		\item There’s an upper bound for message communication but the programmer doesn’t know it – it has to be discovered.
		\item Protocols will operate correctly only if all messages are received within some time, \textit{T}.
		\begin{itemize}
			\item We cannot make assumptions on the delay time distribution
		\end{itemize}
	\end{itemize}
	
\end{slide}

\begin{slide}

	\slidetitle{Latency}
	
	Asynchronous network model
	\begin{itemize}
		\item Messages can take arbitrarily long to reach a peer node.
		\item This is what we get from the Internet!
	\end{itemize}
	\bigskip
	
	Asynchronous networks can be a pain.
	\bigskip
	
	Messages may take an unpredictable amount of time.
	\begin{itemize}
		\item We may think a message is lost but it’s really delayed.
		\item May lead to retransmissions -> duplicate messages
		\item May lead us to assume a service is dead when it isn’t.
		\item May cause messages to arrive in a different order.
	\end{itemize}
	
\end{slide}

\begin{slide}

	\slidetitle{Latency}
	
	Speed up data access via caching – temporary copies of data.
	\bigskip
	
	Keep data close to where it’s processed to maximize efficiency.
	\begin{itemize}
		\item Memory vs. disk.
		\item Local disk vs. remote server.
		\item Remote memory vs. remote disk.
	\end{itemize}
	\bigskip
	
	Cache coherence: cached data can become stale.

\end{slide}

\begin{slide}

	\slidetitle{Partial Failure}
	
	Failure is a fact of life in distributed systems!
	\bigskip
	
	In local systems, failure is usually total (all-or-nothing).
	\bigskip
	
	In distributed systems, we get partial failure.
	\begin{itemize}
		\item A component can fail while others continue to work.
		\item Failure of a network link is indistinguishable from a remote server failure.
	\end{itemize}
	\bigskip
	
	No global state.
	\begin{itemize}
		\item There is no global state that can be examined to determine errors.
		\item There is no agent that can determine which components failed and inform everyone else.
	\end{itemize}
	
\end{slide}

\begin{slide}

	\slidetitle{Handling Failure}
	
	Handle detection, recovery, and restart.
	\bigskip
	
	Availability = fraction of time system is usable.
	\begin{itemize}
		\item Achieve with redundancy.
		\item But keeping all copies consistent is an issue!
	\end{itemize}
	\bigskip
	
	Reliability is how long the system can run without failing.
	\begin{itemize}
		\item Includes ensuring data does not get lost.
		\item includes security.
	\end{itemize}
	\bigskip
	
	If a system recovers quickly from filure, is it highly available? is it highly reliable?
	
\end{slide}

\begin{slide}
	
	\slidetitle{Increasing availability through redundancy}
	
	Redundancy comes from repliated components.
	\begin{itemize}
		\item Service can run even if some systems die.
	\end{itemize}
	\bigskip
	
	If the probability that any one system down is 5\%, what is the probability that two systems are down at the same time?
	\bigskip
	
	What is the uptime in both cases?
	
\end{slide}

\begin{slide}

	\slidetitle{Security}
	
	Traditionally managed by operating systems
	\begin{itemize}
		\item Users authenticate themselves to the system
		\item Each user has a unique user ID (UID)
		\item Access permissions = \textit{f}(UID)
	\end{itemize}
	\bigskip
	
	Now applications must take responsibility for
	\begin{itemize}
		\item Identification
		\item Authentication
		\item Access control
		\item Encryption, tamper detection
		\item Audit trail
	\end{itemize}
	
\end{slide}

\begin{slide}
	
	\slidetitle{Security}
	
	The environment:
	\begin{itemize}
		\item Public networks, remotely-managed services, 3rd party services
		\item Trust: do you trust how the 3rd party services are written \& managed?
	\end{itemize}
	\bigskip
	
	Some issues:
	\begin{itemize}
		\item Malicious interference, bad user input, impersonation of users \& services
		\item Protocol attacks, input validation attacks, time-based attacks, replay attacks
	\end{itemize}
	\bigskip
	
	Rely on cryptography (hashes, cryptography) for identity management, authentication, encryption, tamper detection
	\bigskip
	
	User also wants convenience.
	\begin{itemize}
		\item Single sign-on, no repeated entering of login credentials
	\end{itemize}
	
\end{slide}

\begin{slide}

	\slidetitle{Other considerations}
	
	Scaling up and scaling down
	\begin{itemize}
		\item Need to be able to add and remove components
		\item Impacts failure handling
		\begin{itemize}
			\item If failed components are removed, the system should still work
			\item If replacements are brought in, the system should integrate them
		\end{itemize}
	\end{itemize}
	\bigskip
	
	Algorithms and environment
	\begin{itemize}
		\item Distributable vs. centralized algorithms
		\item Programming languages
		\item APIs and frameworks
	\end{itemize}

\end{slide}

\begin{slide}

	\slidetitle{Service Models - Centralized}
	
	No networking
	\bigskip
	
	Traditional time-sharing system
	\bigskip
	
	Single workstation or PC or direct connection of multiple terminals to a computer.
	\bigskip
	
	One or several CPUs.
	\bigskip
	
	Not easily scalable.

\end{slide}

\begin{slide}

    \slidetitle{Service Models - The client-server model}

    \includegraphics[width=80mm]{client-server-model.png}
    \bigskip

    Clients send requests to servers
	\bigskip
	
	A server is a system that runs a service
	\bigskip
	
	Clients do not communicate with each other.

\end{slide}

\begin{slide}

	\slidetitle{Multi-tier architecture in networked systems}
	
	\includegraphics[width=90mm]{multi-tier-concept.png}
	
	Each tier
	\begin{itemize}
		\item runs as a network service
		\item is accessed by surrounding tiers
	\end{itemize}

\end{slide}

\begin{slide}

	\slidetitle{Multi-tier example}
	
	\includegraphics[width=110mm]{multi-tier-example.png}
	
\end{slide}

\begin{slide}
	
	\slidetitle{Service Models - microservices model}
	
	\begin{minipage}{0.48\textwidth}
		\includegraphics[width=60mm]{microservice-model.png}
	\end{minipage}
	\hfill
	\begin{minipage}{0.5\textwidth}
		Collection of autonomous services
		\bigskip
		
		Each service:
		\begin{itemize}
			\item Runs independently
			\item Has a well-defined interface
			\item May be shared by multiple applications
		\end{itemize}
		\bigskip
		
		Main application coordinates interactions
	\end{minipage}
	
\end{slide}

\begin{slide}

	\slidetitle{Service Models - peer-to-peer model (P2P)}
	
	\begin{minipage}{0.66\textwidth}
		No reliance on servers
		\bigskip
		
		Machines (peers) communicate with each other
		\bigskip
		
		Goals: Robustness and self-scalability
		\bigskip
		
		Examples: BitTorrent, Skype
	\end{minipage}
	\hfill
	\begin{minipage}{0.3\textwidth}
		\includegraphics[width=25mm]{p2p.png}
	\end{minipage}
	
\end{slide}

\begin{slide}

	\slidetitle{Service Models - hybrid model}
	
	Many peer-to-peer architectures still rely on a server
	\begin{itemize}
		\item Look up, track users
		\item Track content
		\item Coordinate access
	\end{itemize}
	\bigskip
	
	But traffic-intensive workloads are delegated to peers
	\medskip
	
	\includegraphics[width=80mm]{hybrid-model.png}

\end{slide}

\begin{slide}

	\slidetitle{Cloud computing}
	
	Resources are provided as a network (internet) service
	\medskip
	
	Software as a Service (SaaS)
	\begin{itemize}
		\item Remotely hosted software: email, productivity, games, ...
		\item Salesforce.com, Google Apps, Microsoft 365
	\end{itemize}
	\medskip
	
	Platform as a Service (PaaS)
	\begin{itemize}
		\item Execution runtimes, databases, web servers, development environments, ...
		\item Google App Engine, AWS Elastic Beanstalk
	\end{itemize}
	\medskip
	
	Infrastructure as a Service (IaaS)
	\begin{itemize}
		\item Computer + storage + networking: VMs, storage servers, load balancers
		\item Microsoft Azure, Google Compute Engine, Amazon Web Services
	\end{itemize}
	\medskip
	
	Storage
	\begin{itemize}
		\item remote file storage
		\item Dropbox, Box, Google Drive, OneDrive, ...
	\end{itemize}
	

\end{slide}
    
\end{document}
